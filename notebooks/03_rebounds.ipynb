{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd176caa-44e5-4fa7-b2aa-f997fca1c382",
   "metadata": {},
   "source": [
    "# Analyzing Recovery Times from Financial Crises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf38fda-0465-4179-8428-987ddf1bec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cedf5b-04b3-4449-b195-ee2de09cadfd",
   "metadata": {},
   "source": [
    "## Downloading and Exploring Data\n",
    "\n",
    "Import [S&P500](https://finance.yahoo.com/quote/%5EGSPC) data from Yahoo Finance. The ticker is \"^GSPC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db544d9d-9797-4a5c-b68f-0b883d46c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../data/gspc.csv\"\n",
    "\n",
    "try:\n",
    "    df_sp500 = pd.read_csv(file)\n",
    "    print(\"Read historical data from\", file) \n",
    "except FileNotFoundError:\n",
    "    df_sp500 = yf.download(\"^GSPC\", start=\"1900-01-01\", multi_level_index=False, auto_adjust=True)\n",
    "    df_sp500.to_csv(file)\n",
    "    print(\"Dowloaded historic data and wrote them into\", file)\n",
    "\n",
    "df_sp500.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f17a0-af3e-46d9-b9c9-db86066053d7",
   "metadata": {},
   "source": [
    "Preliminary data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fe52a-3374-41fe-8228-110777c2b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78553389-d4f8-44dd-8623-fbdd6ef6df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfe0d6-045f-4c67-aa39-6ce4c725de2d",
   "metadata": {},
   "source": [
    "The _Open_ column presents some zero values, which is unusual for financial data and probably denotes missing values. Let's investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fe9f6-537d-4350-a8ce-1477b3d5f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = sum(df_sp500[\"Open\"] == 0)\n",
    "print(f\"There are {zeros} zeros in this column:\")\n",
    "df_sp500[\"Open\"].plot()\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"S&P 500 Open\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e457da-0920-47e9-a6d1-a55d56a553ea",
   "metadata": {},
   "source": [
    "Notice that the historical record of opening values is incomplete, but it becomes more reliable starting in the early 1980s, thanks to advancements in trading technology. Much of the pre-1980s data was reconstructed from newspapers, end-of-day reports, or monthly summaries, which often included only high, low, close, and volume. More accurate historical data exists, but it is not available for free in yahoo finance. Missing data is filled with **zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d77b6-b612-4f7f-b3d6-cb2f32511339",
   "metadata": {},
   "source": [
    "Let's plot _Close_ values, which we expect to be more reliable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd033f02-ec33-48dd-b644-23f437008abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500[\"Close\"].plot()\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"S&P 500 Close\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e96d7-f430-4c15-8661-59fd420857b4",
   "metadata": {},
   "source": [
    "To create more complex analyses or representations, like candlestick graphs, we can shorten our time series considering only post-1985 data (when open values where recorded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32810928-6b57-49b1-99e0-7244f928da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "candlestick = df_sp500[df_sp500.index > \"2000-01-01\"]\n",
    "candlestick.reset_index(inplace=True)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Candlestick(\n",
    "        x=candlestick[\"Date\"],\n",
    "        open=candlestick['Open'],\n",
    "        high=candlestick['High'],\n",
    "        low=candlestick['Low'],\n",
    "        close=candlestick['Close']\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='S&P 500 Candlestick Graph with Rangeslider'),\n",
    "    yaxis=dict(title=dict( text='S&P500 Index'))\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ff1ed-2342-44c9-b84b-6b50588d2c80",
   "metadata": {},
   "source": [
    "## Market Recovery Times \n",
    "\n",
    "We want to evaluate market recovery times. We start by creating some utility columns. In _Previous Max Close_ we store cumulative max values from the _Close_ column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6d4d0-4645-408b-a934-c6f66b254a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500[\"Previous Max Close\"] = df_sp500[\"Close\"].cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4a00d-5c53-4021-acf2-5dd82dc9c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500.loc[:, [\"Close\", \"Previous Max Close\"]].plot()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cc79e-f72a-4281-8ba4-b8e98391e47c",
   "metadata": {},
   "source": [
    "The cummax method is useful, but we'd like to keep track of the date where the previous max occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf36d6-39d2-4d21-ae19-2c5bb360ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask specifying where a new max occurs\n",
    "is_new_max = df_sp500[\"Close\"] == df_sp500[\"Previous Max Close\"]\n",
    "# Find the corresponding dates. This creates a DateTimeIndex with NaT where is_new_max is False\n",
    "new_max_dates = df_sp500.index.where(is_new_max)\n",
    "# Forward-fill the last max date\n",
    "last_max_dates = pd.Series(new_max_dates).ffill()\n",
    "# Align index\n",
    "last_max_dates.index = df_sp500.index\n",
    "\n",
    "df_sp500[\"Previous Max Close Date\"] = last_max_dates\n",
    "df_sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7e2e0-084f-45d9-bd65-b667b2d3b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_days = df_sp500[\"Previous Max Close Date\"].value_counts()\n",
    "\n",
    "# Values counts are already sorted\n",
    "recovery_days = recovery_days[recovery_days.iloc[:] > 90]\n",
    "recovery_days = recovery_days.reset_index()\n",
    "recovery_days.columns = [\"Crash Date\", \"Length (trading days)\"]\n",
    "recovery_days[\"Length (years)\"] = (recovery_days[\"Length (trading days)\"] / 251).map(lambda x : round(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cc325-f335-4324-8e03-dba1f6a6e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_days.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3c56b-85c1-4f89-b9ab-71f9f350d7d0",
   "metadata": {},
   "source": [
    "This is a very rudimental indication of market crashes, definied as periods between local max values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
